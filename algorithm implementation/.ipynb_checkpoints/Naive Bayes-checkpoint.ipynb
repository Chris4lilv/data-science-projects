{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dummy comments from a basketball fan website.\n",
    "\"\"\"\n",
    "def loadDataset():\n",
    "    commentsList = [['I','think','Lebron','James','is','the','GOAT'],\n",
    "                   ['He','is','a','player','like','trash','can'],\n",
    "                   ['I','love','Kyrie','and','his','ball','handling','skill'],\n",
    "                   ['Toronto','Raptors','is','the','team','full','of','trash'],\n",
    "                   ['Nothing','can','express','my','respect','to','Kobe'],\n",
    "                   ['You','trash','stop','talking','like','garbage','man']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return commentsList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create my vocabulary using the dataset above.\n",
    "\"\"\"\n",
    "def createVocabList(dataset):\n",
    "    vocabSet = set([])\n",
    "    for document in dataset:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vocabList: contains all words in our dictionary\n",
    "inputset: random document\n",
    "\"\"\"\n",
    "def bagOfWords2Vec(vocabList, inputset):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputset:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else:\n",
    "            print('The word %s is not in the dictionary!'%word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsList, classVec = loadDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocab = createVocabList(commentsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWords2Vec(myVocab, commentsList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords);p1Num = ones(numWords)\n",
    "    p0Denum = 2.0;p1Denum = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denum += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denum += sum(trainMatrix[i])\n",
    "    p0Vec = log(p0Num / p0Denum)\n",
    "    p1Vec = log(p1Num / p1Denum)\n",
    "    return p0Vec, p1Vec, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add data into training matrix\n",
    "trainMat = []\n",
    "for comment in commentsList:\n",
    "    trainMat.append(bagOfWords2Vec(myVocab, comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0Vec, p1Vec, pAb = trainNB0(trainMat, classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'the',\n",
       " 'He',\n",
       " 'a',\n",
       " 'my',\n",
       " 'Kobe',\n",
       " 'Lebron',\n",
       " 'Nothing',\n",
       " 'skill',\n",
       " 'talking',\n",
       " 'full',\n",
       " 'player',\n",
       " 'is',\n",
       " 'his',\n",
       " 'handling',\n",
       " 'trash',\n",
       " 'to',\n",
       " 'team',\n",
       " 'love',\n",
       " 'respect',\n",
       " 'James',\n",
       " 'like',\n",
       " 'Raptors',\n",
       " 'garbage',\n",
       " 'of',\n",
       " 'ball',\n",
       " 'think',\n",
       " 'man',\n",
       " 'Toronto',\n",
       " 'stop',\n",
       " 'GOAT',\n",
       " 'express',\n",
       " 'I',\n",
       " 'can',\n",
       " 'Kyrie',\n",
       " 'and']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.17805383, -2.48490665, -3.17805383, -3.17805383, -2.48490665,\n",
       "       -2.48490665, -2.48490665, -2.48490665, -2.48490665, -3.17805383,\n",
       "       -3.17805383, -3.17805383, -2.48490665, -2.48490665, -2.48490665,\n",
       "       -3.17805383, -2.48490665, -3.17805383, -2.48490665, -2.48490665,\n",
       "       -2.48490665, -3.17805383, -3.17805383, -3.17805383, -3.17805383,\n",
       "       -2.48490665, -2.48490665, -3.17805383, -3.17805383, -3.17805383,\n",
       "       -2.48490665, -2.48490665, -2.07944154, -2.48490665, -2.48490665,\n",
       "       -2.48490665])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.48490665, -2.48490665, -2.48490665, -2.48490665, -3.17805383,\n",
       "       -3.17805383, -3.17805383, -3.17805383, -3.17805383, -2.48490665,\n",
       "       -2.48490665, -2.48490665, -2.07944154, -3.17805383, -3.17805383,\n",
       "       -1.79175947, -3.17805383, -2.48490665, -3.17805383, -3.17805383,\n",
       "       -3.17805383, -2.07944154, -2.48490665, -2.48490665, -2.48490665,\n",
       "       -3.17805383, -3.17805383, -2.48490665, -2.48490665, -2.48490665,\n",
       "       -3.17805383, -3.17805383, -3.17805383, -2.48490665, -3.17805383,\n",
       "       -3.17805383])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trash'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocab[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2classify * p0Vec) + log(1.0 - pClass1)\n",
    "    p2 = sum(vec2classify * p1Vec) + log(pClass1)\n",
    "    if p1 > p2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNB():\n",
    "    commentsList, classVec = loadDataset()\n",
    "    myVocab = createVocabList(commentsList)\n",
    "    trainMat = []\n",
    "    for comment in commentsList:\n",
    "        trainMat.append(bagOfWords2Vec(myVocab, comment))\n",
    "    p0Vec, p1Vec, pClass1 = trainNB0(trainMat, classVec)\n",
    "    testEntry0 = ['love','my','dalmation']\n",
    "    thisDoc = array(bagOfWords2Vec(myVocab, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0Vec, p1Vec, pClass1))\n",
    "    testEntry1 = ['trash','garbage']\n",
    "    thisDoc = array(bagOfWords2Vec(myVocab, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0Vec, p1Vec, pClass1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
